I"¬n<p>Recently I was tasked with implemented a block LU decomposition in parallel using
a block cyclic process distribution using BLACS and MPI. This decomposition would
then be extended to hierarchical matrices and would eventually work with dense matrices
instead of hierarchical. Thus we cannot use already implemented distributed LU factorization
methods like scalapack for this purpose.</p>

<p>In this post I would like to document my learnings about desinging the parallel algorithm
and installing the various libraries that are required for this purpose. Hopefully, the reader
will find something useful in this post too. This post will cover only LU factorization of dense
matrices. Hierarchical matrices will be covered in another post.</p>

<p>I have written about using the scalapack C++ interface for a simple block LU decomposition 
in <a href="URL">this</a> post.</p>

<!-- markdown-toc start - Don't edit this section. Run M-x markdown-toc-generate-toc again -->
<p><strong>Table of Contents</strong></p>

<ul>
  <li><a href="#installing-libraries">Installing libraries</a></li>
  <li><a href="#designing-the-algorithm">Designing the algorithm</a>
    <ul>
      <li><a href="#asynchronous-block-lu">Asynchronous block LU</a></li>
      <li><a href="#synchronous-block-lu">Synchronous block LU</a></li>
      <li><a href="#resources">Resources</a></li>
    </ul>
  </li>
  <li><a href="#implementation-with-mpi">Implementation with MPI</a>
    <ul>
      <li><a href="#block-cyclic-data-distribution">Block cyclic data distribution</a>
        <ul>
          <li><a href="#block-cyclic-nomenclature">Block cyclic nomenclature</a></li>
        </ul>
      </li>
      <li><a href="#scalapack-protips">ScaLAPACK protips</a>
        <ul>
          <li><a href="#use-of-m-and-n-in-routines">Use of M and N in routines</a></li>
        </ul>
      </li>
      <li><a href="#blacs-protips">BLACS protips</a>
        <ul>
          <li><a href="#blacs-topologies">BLACS topologies</a></li>
          <li><a href="#blacs-general-apis">BLACS general APIs</a></li>
        </ul>
      </li>
      <li><a href="#asynchronous-block-lu">Asynchronous block LU</a></li>
      <li><a href="#synchronous-block-lu">Synchronous block LU</a></li>
    </ul>
  </li>
  <li><a href="#resources">Resources</a>
    <ul>
      <li><a href="#blacs">BLACS</a></li>
    </ul>
  </li>
</ul>

<!-- markdown-toc end -->

<h1 id="installing-libraries">Installing libraries</h1>

<p>For this computation, we use MPICH and <a href="">BLACS</a>. While MPICH is easily installable on most
GNU/Linux distributions, the same cannot be said for BLACS.</p>

<p>I first tried downloading <a href="">BLACS sources</a> and compiling the library, however it gave too
many compilation errors and was taking a long time to debug. Therefore, I resorted to using
the <a href="">ScaLAPACK installer</a>, which is a Python script that downloads the sources of BLACS,
LAPACK and ScaLAPACK, compiles all these libraries on your system and produces a single 
shared object file <code>libscalapack.a</code> which you can use for linking with your program. 
Since BLACS is included in the ScaLAPACK distribution, you can use the scalapack binary
directly for linking.</p>

<p>Just download the ScaLAPACK installer from the website and follow the instructions in the README for quick and easy installation.</p>

<h1 id="designing-the-algorithm">Designing the algorithm</h1>

<h2 id="asynchronous-block-lu">Asynchronous block LU</h2>

<p>One problem that I faced when designing the algorithm is that when writing a CBLACS
program, you are basically writing the same code that is being run on multiple processes, 
however the data that is stored in variables is not the same for each process.</p>

<p>So it becomes important to write the program in such a way that maximum data is shared
between the processes but there is minimmum communication of things like the block
that is currently under process.</p>

<p>If it is a diagonal block, it simply factorizes the block into L &amp; U parts and broadcasts
it to rows and columns.</p>

<p>If it is a row or column block, it listens for the broadcast from the diagonal block and mutliplies the contents that it receives with the data it posseses. It then broadcasts the multiplied matrix block accross the lower right block so that the block can be reduced.</p>

<p>It can be expressed with this line of code:</p>
<div class="language-cpp highlighter-coderay"><table class="CodeRay"><tr>
  <td class="line-numbers"><pre><a href="#n1" name="n1">1</a>
</pre></td>
  <td class="code"><pre>p2p_recv(recv_block, blocksize, rows[index] % N, rows[index] % N);
</pre></td>
</tr></table>
</div>

<p>The source row and source col arguments (last two) are computed by keeping in mind
that we can compute the diagonal block of a particular block if we know the absolute
row number of the block.</p>

<p>If is a block in the right lower block of the matrix (the A^ block), it waits for 
the broadcast from the row and column elements, multiplies the received data with
the stored data and over writes the stored data.</p>

<p>The computation and communication is mostly asynchronous. This means that there
needs to be some kind of a trigger to launch the computation or communication
tasks in a given process.</p>

<p>A major problem is synchronization of successive diagonal matrix blocks. The
computation must proceed from the top left corner of the matrix until the lower
right corner. For this to work properly it is important that the diagonal blocks
do not compute and send their data unless the diagonal block to the upper left 
of the block has finished computing.</p>

<h2 id="synchronous-block-lu">Synchronous block LU</h2>

<p>The main thing to take care of in synchronous block LU is that of the indexing of the data array
and the subsequent generation of the matrix. To demonstrate, here is what the matrix structure of
the synchronous block LU looks like:</p>

<!-- insert that hand drawn image of sync block LU here -->

<p>We can know the actual row and col number of the global matrix through the process ID and the
block number. The following lines of code are useful for this purpose:</p>
<div class="language-cpp highlighter-coderay"><table class="CodeRay"><tr>
  <td class="line-numbers"><pre><a href="#n1" name="n1">1</a>
<a href="#n2" name="n2">2</a>
<a href="#n3" name="n3">3</a>
<a href="#n4" name="n4">4</a>
<a href="#n5" name="n5">5</a>
<a href="#n6" name="n6">6</a>
<a href="#n7" name="n7">7</a>
<a href="#n8" name="n8">8</a>
<a href="#n9" name="n9">9</a>
<strong><a href="#n10" name="n10">10</a></strong>
</pre></td>
  <td class="code"><pre><span style="color:#777">// bcounter_i is a counter identifying the block row within a process</span>
<span style="color:#777">// bcounter_j is a counter identifying the block col within a process</span>
<span style="color:#777">// num_blocks_per_process is the number of blocks in a process</span>
<span style="color:#777">// myrow is the BLACS process row number</span>
<span style="color:#777">// mycol is the BLACS process col number</span>
<span style="color:#777">// block_size_per_process_r is the row size of each block within the process</span>
<span style="color:#777">// block_size_per_process_c is the col size of each block within the process</span>

row_i = bcounter_i*num_blocks_per_process + myrow*block_size_per_process_r + i;
col_j = bcounter_j*num_blocks_per_process + mycol*block_size_per_process_c + j;
</pre></td>
</tr></table>
</div>

<p>We can get the index number of the data array in the following manner:</p>
<div class="language-cpp highlighter-coderay"><table class="CodeRay"><tr>
  <td class="line-numbers"><pre><a href="#n1" name="n1">1</a>
<a href="#n2" name="n2">2</a>
</pre></td>
  <td class="code"><pre><span style="color:#0a8;font-weight:bold">int</span> index = (bcounter_i*block_size_per_process_r + bcounter_j)*
    num_blocks_per_process +  i*process_block_size + j;
</pre></td>
</tr></table>
</div>

<p>Before creating a full-fledged version of this code, I first made a simple code
that would calculate the LU decomposition in the case where there is only one
matrix block per process.</p>

<h2 id="resources">Resources</h2>

<p>Some resources that I found during this phase are as follows:</p>
<ul>
  <li><a href="http://www.mcs.anl.gov/~itf/dbpp/">Designing and building parallel programs.</a></li>
  <li><a href="http://www-users.cs.umn.edu/~karypis/parbook/">Introduction to Parallel Computing.</a></li>
  <li><a href="https://computing.llnl.gov/tutorials/parallel_comp/#Designing">Designing parallel programs course.</a></li>
  <li><a href="http://people.eecs.berkeley.edu/~demmel/cs267/lecture12/lecture12.html">Lecture on parallel Gaussian from Berkeley</a>.</li>
  <li><a href="https://cseweb.ucsd.edu/classes/sp07/cse262/Projects/260_fa06/Ricketts_SR.pdf">Parallelizing LU factorization.</a></li>
</ul>

<h1 id="implementation-with-mpi">Implementation with MPI</h1>

<p>Each process should hold only the part of the matrix that it is working upon.</p>

<h2 id="block-cyclic-data-distribution">Block cyclic data distribution</h2>

<p>The block cyclic distribution is a central idea in the case of PBLAS and BLACS.
It is important to store the matrix in this configuration since it is the most 
efficient in terms of load balancing for most applications.</p>

<p>If youâ€™re reading a matrix from an external file it can get cumbersome to read 
into in a block cyclic manner manually. You do this with little effort using MPI IO.
Refer <a href="URL">this blog post</a> that describes this in detail along with C code.</p>

<p>For this code we generate the data on a per process basis.</p>

<h3 id="block-cyclic-nomenclature">Block cyclic nomenclature</h3>

<p>Its somewhat confusing how exactly the blocks are named. So hereâ€™s the nomenclature
Iâ€™m using when talking about certain kinds of blocks:</p>
<ul>
  <li>Process blocks :: blocks inside a process.</li>
  <li>Matrix blocks :: blocks of the global matrix.</li>
  <li>Matrix sub-blocks :: Each matrix block is divided into sub-blocks that are scattered
over processes. Each of these sub-blocks corresponds to a single process block.</li>
</ul>

<h2 id="mpi-communication-protips">MPI communication protips</h2>

<h3 id="communicating-lower-triangular-matrices-with-mpi_type_indexed">Communicating lower triangular matrices with <code>MPI_Type_indexed</code></h3>

<p>For communicating arrays that are not contiguos in memory it is useful to use the
<code>MPI_Type_indexed</code> function for sending/receiving a non-contiguos array stored in memory.</p>

<p>If using <code>indexed</code> for sending, one must keep in mind that the array will be sent and
received in the exact same form that it is sent. So for example, if you have a 4x4 matrix
stored in an array of length 16 and you wish to send the lower triangle of this array,
you will need to reserve an array of length 16 at the receiving process too. The receiving
array will be populated at the positions that are indicated by the displacement and length
arrays that you commit when making the data type.</p>

<p>Hereâ€™s a sample code for sending the lower triangle part of a 4x4 matrix stored as a 1D array:</p>
<div class="language-cpp highlighter-coderay"><table class="CodeRay"><tr>
  <td class="line-numbers"><pre><a href="#n1" name="n1">1</a>
<a href="#n2" name="n2">2</a>
<a href="#n3" name="n3">3</a>
<a href="#n4" name="n4">4</a>
<a href="#n5" name="n5">5</a>
<a href="#n6" name="n6">6</a>
<a href="#n7" name="n7">7</a>
<a href="#n8" name="n8">8</a>
<a href="#n9" name="n9">9</a>
<strong><a href="#n10" name="n10">10</a></strong>
<a href="#n11" name="n11">11</a>
<a href="#n12" name="n12">12</a>
<a href="#n13" name="n13">13</a>
<a href="#n14" name="n14">14</a>
<a href="#n15" name="n15">15</a>
<a href="#n16" name="n16">16</a>
<a href="#n17" name="n17">17</a>
<a href="#n18" name="n18">18</a>
<a href="#n19" name="n19">19</a>
<strong><a href="#n20" name="n20">20</a></strong>
<a href="#n21" name="n21">21</a>
<a href="#n22" name="n22">22</a>
<a href="#n23" name="n23">23</a>
<a href="#n24" name="n24">24</a>
<a href="#n25" name="n25">25</a>
<a href="#n26" name="n26">26</a>
<a href="#n27" name="n27">27</a>
<a href="#n28" name="n28">28</a>
<a href="#n29" name="n29">29</a>
<strong><a href="#n30" name="n30">30</a></strong>
<a href="#n31" name="n31">31</a>
<a href="#n32" name="n32">32</a>
<a href="#n33" name="n33">33</a>
<a href="#n34" name="n34">34</a>
<a href="#n35" name="n35">35</a>
<a href="#n36" name="n36">36</a>
<a href="#n37" name="n37">37</a>
<a href="#n38" name="n38">38</a>
<a href="#n39" name="n39">39</a>
<strong><a href="#n40" name="n40">40</a></strong>
<a href="#n41" name="n41">41</a>
<a href="#n42" name="n42">42</a>
<a href="#n43" name="n43">43</a>
<a href="#n44" name="n44">44</a>
<a href="#n45" name="n45">45</a>
<a href="#n46" name="n46">46</a>
</pre></td>
  <td class="code"><pre><span style="color:#777">// Sample program for demoing sending the lower triangle of a square</span>
<span style="color:#777">// matrix using types made by the MPI_Type_indexed function.</span>

<span style="color:#579">#include</span> <span style="color:#B44;font-weight:bold">&quot;mpi.h&quot;</span>
<span style="color:#579">#include</span> <span style="color:#B44;font-weight:bold">&lt;iostream&gt;</span>
<span style="color:#088;font-weight:bold">using</span> <span style="color:#080;font-weight:bold">namespace</span> std;

<span style="color:#0a8;font-weight:bold">int</span> main()
{
  MPI_Init(<span style="color:#069">NULL</span>, <span style="color:#069">NULL</span>);
  <span style="color:#0a8;font-weight:bold">int</span> mpi_rank, mpi_size;
  MPI_Comm_size(MPI_COMM_WORLD, &amp;mpi_size);
  MPI_Comm_rank(MPI_COMM_WORLD, &amp;mpi_rank);

  <span style="color:#0a8;font-weight:bold">double</span> A[<span style="color:#00D">16</span>], G[<span style="color:#00D">16</span>];
  <span style="color:#0a8;font-weight:bold">int</span> displs[<span style="color:#00D">4</span>] = {<span style="color:#00D">0</span>, <span style="color:#00D">4</span>, <span style="color:#00D">8</span>, <span style="color:#00D">12</span>};
  <span style="color:#0a8;font-weight:bold">int</span> lens[<span style="color:#00D">4</span>] = {<span style="color:#00D">1</span>, <span style="color:#00D">2</span>, <span style="color:#00D">3</span>, <span style="color:#00D">4</span>};
  MPI_Datatype tril;
  MPI_Status status;

  <span style="color:#080;font-weight:bold">for</span> (<span style="color:#0a8;font-weight:bold">int</span> i = <span style="color:#00D">0</span>; i &lt; <span style="color:#00D">16</span>; i++)
    A[i] = i+<span style="color:#00D">1</span>;

  MPI_Type_indexed(<span style="color:#00D">4</span>, lens, displs, MPI_DOUBLE, &amp;tril);
  MPI_Type_commit(&amp;tril);

  <span style="color:#080;font-weight:bold">if</span> (mpi_rank == <span style="color:#00D">0</span>) {
    MPI_Send(A, <span style="color:#00D">1</span>, tril, <span style="color:#00D">1</span>, <span style="color:#00D">0</span>, MPI_COMM_WORLD);
  }

  <span style="color:#080;font-weight:bold">if</span> (mpi_rank == <span style="color:#00D">1</span>) {
    MPI_Recv(G, <span style="color:#00D">1</span>, tril, <span style="color:#00D">0</span>, <span style="color:#00D">0</span>, MPI_COMM_WORLD, &amp;status);

    <span style="color:#080;font-weight:bold">for</span> (<span style="color:#0a8;font-weight:bold">int</span> i = <span style="color:#00D">0</span>; i &lt; <span style="color:#00D">4</span>; i++) {
      <span style="color:#080;font-weight:bold">for</span> (<span style="color:#0a8;font-weight:bold">int</span> j = <span style="color:#00D">0</span>; j &lt; <span style="color:#00D">4</span>; j++) {
        <span style="color:#080;font-weight:bold">if</span> (j &lt;= i)
          cout &lt;&lt; G[i*<span style="color:#00D">4</span> + j] &lt;&lt; <span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">&quot;</span><span style="color:#D20"> </span><span style="color:#710">&quot;</span></span>;
        <span style="color:#080;font-weight:bold">else</span>
          cout &lt;&lt; <span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">&quot;</span><span style="color:#D20"> </span><span style="color:#710">&quot;</span></span>;
      }
      cout &lt;&lt; endl;
    }
  }
  MPI_Type_free(&amp;tril);
  MPI_Finalize();
}
</pre></td>
</tr></table>
</div>

<h3 id="communicating-lower-triangular-matrices-using-mpi_pack">Communicating lower triangular matrices using MPI_Pack.</h3>

<p>Link:</p>

<ul>
  <li>https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwj37KWzyNfcAhWCQN4KHcXACzUQFjACegQICBAC&amp;url=http%3A%2F%2Fwww.mathnet.or.kr%2Fmathnet%2Fpaper_file%2Fiowa%2FGlenn%2FMPI.Derived.Types.Yanmei.April14.2005.doc&amp;usg=AOvVaw3Qbf0Sbs6SWcAuddAZrFqG</li>
</ul>

<h2 id="scalapack-protips">ScaLAPACK protips</h2>

<h3 id="use-of-m-and-n-in-routines">Use of M and N in routines</h3>

<p>ScaLAPACK operates on a block cyclic data distribution. Most of the routines accept
two parameters: <code>M</code> and <code>N</code> that are described as the number of rows and cols of the
distributed submatrix sub(A). Its easy to get confused by thinking of these variables
as the dimensions of the <em>global</em> matrix. However, since scalapack relies on a block
cyclic data distribution, the â€˜worldâ€™ for all processes at <em>one</em> time is basically one
matrix block which is spread over all the processes. Therefore, when calling scalapack
routines care must be taken to specify the dimensions of the matrix block in <code>M</code> and <code>N</code>
and not those of the global matrix.</p>

<p>If you see other code that does not rely on multiple sub-matrix blocks inside processes, 
they will usually pass the dimensions of the global matrix to the routine, which is correct
for that case since there is only one sub-matrix block per process.</p>

<h2 id="blacs-protips">BLACS protips</h2>

<h3 id="blacs-topologies">BLACS topologies</h3>

<h3 id="blacs-general-apis">BLACS general APIs</h3>

<p>Similar to MPI, BLACS contains some routines for sending and receiving data in 
a point-to-point manner. They are as below:</p>
<ul>
  <li><code>gesd2d</code>: This routine is for point-to-point sending of data from one process to another. This routine is non-blocking by default (unlike <code>MPI_Send</code> which is blocking). Itâ€™s prototype for the C interface is as follows:
    <div class="language-cpp highlighter-coderay"><table class="CodeRay"><tr>
  <td class="line-numbers"><pre><a href="#n1" name="n1">1</a>
<a href="#n2" name="n2">2</a>
<a href="#n3" name="n3">3</a>
<a href="#n4" name="n4">4</a>
<a href="#n5" name="n5">5</a>
<a href="#n6" name="n6">6</a>
<a href="#n7" name="n7">7</a>
<a href="#n8" name="n8">8</a>
<a href="#n9" name="n9">9</a>
</pre></td>
  <td class="code"><pre><span style="color:#088;font-weight:bold">void</span> Cdgesd2d(
  <span style="color:#0a8;font-weight:bold">int</span> CBLACS_CONTEXT, <span style="color:#777">// CBLACS context</span>
  <span style="color:#0a8;font-weight:bold">int</span> M, <span style="color:#777">// row size of matrix block</span>
  <span style="color:#0a8;font-weight:bold">int</span> N, <span style="color:#777">// col size of matrix block</span>
  <span style="color:#0a8;font-weight:bold">double</span>* A, <span style="color:#777">// pointer to matrix block</span>
  <span style="color:#0a8;font-weight:bold">int</span> LDA, <span style="color:#777">// leading dim of A (col size for C programs)</span>
  <span style="color:#0a8;font-weight:bold">int</span> RDEST, <span style="color:#777">// row number of destination process</span>
  <span style="color:#0a8;font-weight:bold">int</span> CDEST <span style="color:#777">// col number of destination process</span>
);
</pre></td>
</tr></table>
    </div>
  </li>
  <li><code>trsd2d</code>: This routine is used for point-to-point sending of trapezoidal matrices.</li>
  <li><code>gerv2d</code>: This routine is used for point-to-point receiving of general rectangular
matrices. This routine will block until the message is received. Its prototype looks like so:
    <div class="language-cpp highlighter-coderay"><table class="CodeRay"><tr>
  <td class="line-numbers"><pre><a href="#n1" name="n1">1</a>
<a href="#n2" name="n2">2</a>
<a href="#n3" name="n3">3</a>
<a href="#n4" name="n4">4</a>
<a href="#n5" name="n5">5</a>
<a href="#n6" name="n6">6</a>
<a href="#n7" name="n7">7</a>
<a href="#n8" name="n8">8</a>
<a href="#n9" name="n9">9</a>
</pre></td>
  <td class="code"><pre><span style="color:#088;font-weight:bold">void</span> Cdgerv2d(
  <span style="color:#0a8;font-weight:bold">int</span> CBLACS_CONTEXT, <span style="color:#777">// CBLACS conntext</span>
  <span style="color:#0a8;font-weight:bold">int</span> M, <span style="color:#777">// row size of matrix block</span>
  <span style="color:#0a8;font-weight:bold">int</span> N, <span style="color:#777">// col size of matrix block</span>
  <span style="color:#0a8;font-weight:bold">double</span> *A, <span style="color:#777">// pointer to matrix data.</span>
  <span style="color:#0a8;font-weight:bold">int</span> LDA, <span style="color:#777">// leading dim of A (col size for C)</span>
  <span style="color:#0a8;font-weight:bold">int</span> RSRC, <span style="color:#777">// process row co-ordinate of the sending process.</span>
  <span style="color:#0a8;font-weight:bold">int</span> CSRC <span style="color:#777">// process col co-ordinate of the sending process.</span>
);
</pre></td>
</tr></table>
    </div>
  </li>
</ul>

<p>For broadcast receive, there is the <code>gebr2d</code> routine. This routine is particularly 
useful since it can broadcast over all processes, or a specific row or column. 
This can be helpful over using MPI directly since it allows us to easily broadcast
over rows or columns without having to define separate communicators.</p>

<p>The prototype of this routine is as follows:</p>
<div class="language-cpp highlighter-coderay"><table class="CodeRay"><tr>
  <td class="line-numbers"><pre><a href="#n1" name="n1">1</a>
<a href="#n2" name="n2">2</a>
<a href="#n3" name="n3">3</a>
<a href="#n4" name="n4">4</a>
<a href="#n5" name="n5">5</a>
<a href="#n6" name="n6">6</a>
<a href="#n7" name="n7">7</a>
<a href="#n8" name="n8">8</a>
<a href="#n9" name="n9">9</a>
<strong><a href="#n10" name="n10">10</a></strong>
<a href="#n11" name="n11">11</a>
<a href="#n12" name="n12">12</a>
<a href="#n13" name="n13">13</a>
<a href="#n14" name="n14">14</a>
</pre></td>
  <td class="code"><pre><span style="color:#777">// Cd stands for 'C double'</span>
<span style="color:#777">// ge is 'general rectangular matrix'</span>
<span style="color:#777">// br is 'broadcast receive'</span>
<span style="color:#088;font-weight:bold">void</span> Cdgebr2d(
    <span style="color:#0a8;font-weight:bold">int</span> CBLACS_CONTEXT, <span style="color:#777">// CBLACS context</span>
    <span style="color:#0a8;font-weight:bold">char</span>* SCOPE, <span style="color:#777">// scope of the broadcast. Can be &quot;Row&quot;, &quot;Column&quot; or &quot;All&quot;</span>
    <span style="color:#0a8;font-weight:bold">char</span>* TOP, <span style="color:#777">// indicates communication pattern to use for broadcast.</span>
    <span style="color:#0a8;font-weight:bold">int</span> M, <span style="color:#777">// number of rows of matrix.</span>
    <span style="color:#0a8;font-weight:bold">int</span> N, <span style="color:#777">// number of columns of matrix.</span>
    <span style="color:#0a8;font-weight:bold">double</span>* A, <span style="color:#777">// pointer to matrix data.</span>
    <span style="color:#0a8;font-weight:bold">int</span> LDA, <span style="color:#777">// leading dim of matrix (col size for C)</span>
    <span style="color:#0a8;font-weight:bold">int</span> RSRC, <span style="color:#777">// process row co-ordinate of the process who called broadcast/send.</span>
    <span style="color:#0a8;font-weight:bold">int</span> CSRC <span style="color:#777">// process column co-ordinate of the process who called broadcast/send.</span>
);
</pre></td>
</tr></table>
</div>

<p>For broadcast send, there is the <code>gebs2d</code> routine. This is helpful for receiving broadcasts.
The prototype of this function is as follows:</p>
<div class="language-cpp highlighter-coderay"><table class="CodeRay"><tr>
  <td class="line-numbers"><pre><a href="#n1" name="n1">1</a>
<a href="#n2" name="n2">2</a>
<a href="#n3" name="n3">3</a>
<a href="#n4" name="n4">4</a>
<a href="#n5" name="n5">5</a>
<a href="#n6" name="n6">6</a>
<a href="#n7" name="n7">7</a>
<a href="#n8" name="n8">8</a>
<a href="#n9" name="n9">9</a>
</pre></td>
  <td class="code"><pre>Cdgebs2d(
    <span style="color:#0a8;font-weight:bold">int</span> CBLACS_CONTEXT, <span style="color:#777">// CBLACS context.</span>
    <span style="color:#0a8;font-weight:bold">char</span>* SCOPE, <span style="color:#777">// scope of broadcast. can be &quot;All&quot;, &quot;Row&quot; or &quot;Column&quot;.</span>
    <span style="color:#0a8;font-weight:bold">char</span>* TOP, <span style="color:#777">// network topology to be used.</span>
    <span style="color:#0a8;font-weight:bold">int</span> M, <span style="color:#777">// num of rows of the matrix.</span>
    <span style="color:#0a8;font-weight:bold">int</span> N, <span style="color:#777">// num of cols of the matrix.</span>
    <span style="color:#0a8;font-weight:bold">double</span> *A, <span style="color:#777">// pointer to the matrix data.</span>
    <span style="color:#0a8;font-weight:bold">int</span> LDA <span style="color:#777">// leading dimension of A.</span>
);
</pre></td>
</tr></table>
</div>
<p>The <code>TOP</code> argument specifies the communication pattern to use. Leave it as a blank space
(<code>" "</code>) to use the default.</p>

<h2 id="asynchronous-block-lu-1">Asynchronous block LU</h2>

<h2 id="synchronous-block-lu-1">Synchronous block LU</h2>

<p>In the asynchronous LU, it is assumed that the block size is equal to the processor size,
i.e each block of the matrix is limited to only a single processor. For synchronous LU 
decomposition, we take blocks which are spread out over multiple processors. To illustrate, 
see the below figure:</p>

<p>Four of the above colors represent a single block and each color represents a process. This
means that each block is spread out over 4 processes. This ensures that the processes are
always kept busy no matter the operation.</p>

<p>It should be remembered that scalapack expects the data to be in column-major format.
Therefore, it must be stored that way.</p>

<h1 id="resources-1">Resources</h1>

<h2 id="blacs">BLACS</h2>

<ul>
  <li><a href="https://software.intel.com/en-us/mkl-developer-reference-c-blacs-routines">Intel MKL BLACS resources</a>.</li>
  <li><a href="https://andyspiros.wordpress.com/2011/07/08/an-example-of-blacs-with-c/">Blog post detailing use of BLACS for scatter operations.</a></li>
  <li><a href="http://www.netlib.org/blacs/BLACS/QRef.html#BS">Netlib BLACS reference</a>.</li>
  <li><a href="http://www.netlib.org/blacs/BLACS/Array.html">BLACS array-based communication</a>.</li>
  <li><a href="http://www.netlib.org/lapack/lawnspdf/lawn94.pdf">BLACS user manual</a>.</li>
  <li><a href="http://www.netlib.org/blacs/BLACS/Top.html">BLACS communication topologies</a>.</li>
  <li><a href="https://scicomp.stackexchange.com/questions/1688/how-do-i-use-scalapack-pblas-for-matrix-vector-multiplication">Using PBLAS for matrix multiplication.</a></li>
  <li><a href="https://software.intel.com/en-us/mkl-developer-reference-c-pblas-routines-overview">PBLAS rountines overview from Intel.</a></li>
  <li><a href="http://www.nersc.gov/users/software/programming-libraries/math-libraries/libsci/libsci-example/">ScaLAPACK pdgemm matrix multiplication example.</a></li>
  <li><a href="http://www.training.prace-ri.eu/uploads/tx_pracetmo/scalable_linear_algebra.pdf">Presentation about Scalapack/PBLAS/BLACS with good details on usage.</a></li>
  <li><a href="http://www.netlib.org/utk/papers/scalapack/node8.html">Block cyclic data distribution (netlib).</a></li>
  <li><a href="http://www.netlib.org/blacs/BLACS/Top.html">BLACS Topology.</a></li>
</ul>
:ET