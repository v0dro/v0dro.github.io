
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>[code]Generalized Linear Models: Introduction and Implementation in Ruby. - Travel &lt;code&gt; Music</title>
  <meta name="author" content="Sameer Deshmukh">

  
  <meta name="description" content="Overview Most of us are well acquainted with linear regression and its use in analysig the relationship of one dataset with another. Linear &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://v0dro.github.io/blog/2014/09/21/code-generalized-linear-models-introduction-and-implementation-in-ruby/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Travel &lt;code&gt; Music" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-55005305-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>



  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$', '$'], ["\(", "\)"] ],
        displayMath: [ ['$$', '$$'], ["\[", "\]"] ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
      //,
      //displayAlign: "left",
      //displayIndent: "2em"
    });
  </script>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Travel &lt;code&gt; Music</a></h1>
  
    <h2>A place where I share my experiences with Travel, Programming and Music</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="v0dro.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/blog/archives">Archive</a></li>
  <li><a href="/travel">Travel</a></li>
  <li><a href="/code">Code</a></li>
  <li><a href="/music">Music</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">[code]Generalized Linear Models: Introduction and Implementation in Ruby.</h1>
    
    
      <p class="meta">
        





        
      </p>
    
  </header>


<div class="entry-content"><h2 id="overview">Overview</h2>

<p>Most of us are well acquainted with linear regression and its use in analysig the relationship of one dataset with another. Linear regression basically shows the (possibly) linear relationship between one or more independent variables and a single dependent variable. But what if this relationship is not linear and the dependent and independent variables are associated with one another through some special function? This is where Generalized Linear Models (or GLMs) come in. This article will explain some core <a href="http://en.wikipedia.org/wiki/Generalized_linear_model">GLM</a> concepts and their implementation in Ruby using the <a href="https://github.com/sciruby/statsample-glm">statsample-glm</a> gem.</p>

<h2 id="generalized-linear-models-basics">Generalized Linear Models Basics</h2>

<p>The basic linear regression equation relating the dependent varible <em>y</em> with the independent variable <em>x</em> looks something like 
<script type="math/tex">\begin{align}
    y = \beta_{0} + x_{1}*\beta_{1} + x_{2}*\beta_{2}...
\end{align}</script>
This is the equation of a straight line, with <script type="math/tex">\beta_{0}</script> denoting the intercept of the line with the Y axis and <script type="math/tex">\beta_{1}</script> denoting the slope of the line. GLMs take this a step further. They try to establish a relationship between <em>x</em> and <em>y</em> through <em>another function</em> <strong>g(x)</strong>, which is called the <em>link function</em>. This function depends on the probability distribution displayed by the independent variables and their corresponding y values. In its simplest form, it can be denoted as <em>y = g(x)</em>.</p>

<p>GLM can be used to model numerous relations, depending on the distribution of the dependent conditional on the independent variables. We will first explore the various kinds of GLMs and their defining parameters and then understand the different methods employed in finding the co-efficients. The most common GLMs are:</p>

<ul>
  <li>Logistic (or logit) regression.</li>
  <li>Normal regression.</li>
  <li>Poisson regression.</li>
  <li>Probit regression.</li>
</ul>

<p>Let’s see all of the above one by one.</p>

<h4 id="logisitic-regression">Logisitic Regression</h4>
<p>Logistic, or Logit can be said to be one of the most fundamental of the GLMs. It is mainly used in cases where the independent variables show a binomial distribution (conditional on the dependent). In case of the binomial distribution, the number of successes are modelled on a fixed number of tries. The Bernoulli distribution is a special case of binomial where the outcome is either 0 or 1 (which is the case in the example at the bottom of this post). By using logit link function, one can determine the maximum probability of the occurence of each independent random variable. The values so obtained can be used to plot a sigmoid graph of <em>x</em> vs <em>y</em>, using which one can predict the probability of occurence of any random varible not already in the dataset. The defining parameter of the logistic is the probability <em>y</em>.</p>

<p>The logit link function looks something like 
<script type="math/tex">\begin{align}
    y = \frac{e^{(\beta_{0} + x*\beta_{1})}}{1 + e^{(\beta_{0} + x*\beta_{1})}}
\end{align}</script>
, where y is the probability for the given value of x.</p>

<p>Of special interest is the meaning of the values of the coefficients. In case on linear regression, <script type="math/tex">\beta_{0}</script> merely denotes the intercept while <script type="math/tex">\beta_{1}</script> is the slope of the line. However, here, because of the nature of the link function, the coefficient <script type="math/tex">\beta_{1}</script> of the independent variable is interpreted as “for every 1 increase in <em>x</em> the odds of <em>y</em> increase by <script type="math/tex">e^{\beta_{1}}</script> times”.</p>

<p>One thing that puzzled me when I started off with regression was the purpose of having several variables <script type="math/tex">(x_{1}, x_{2}...)</script> in the same regression model at times. The purpose of multiple independent variables against a single dependent is so that we can compare the odds of <script type="math/tex">x_{1}</script> against <script type="math/tex">x_{2}</script>.  So basically, if you have multiple variables, it is to compare the effect on the dependent of one variable, when the others are constant. To compare the effect of one variable without considering the others, one could use an  independent regression for each one.</p>

<p>The logistic graph generally looks like this:</p>

<p><img class="center" src="/images/glm/logistic.gif" alt="Generic Graph of Logistic Regression." /></p>

<h4 id="normal-regression">Normal Regression</h4>

<p>Normal regression is used when the DEPENDENT variable exhibits a normal probability distribution, CONDITIONAL ON THE independent variables. The independents are assumed to be normal even in a simple linear or multiple regression, and the coefficients of a normal are more easily calculated using simple linear regression methods. But since this is another very important and commonly found data set, we will look into it.</p>

<p>Normally distributed data is symmetric about the center and its mean is equal to its median. Commonly found normal distributions are heights of people and errors in measurement. The defining parameters of a normal distribution are the mean <script type="math/tex">\mu</script> and variance <script type="math/tex">\sigma^2</script>. The link function is simply <script type="math/tex">y = x*\beta_{1}</script> if no constant is present. The coefficient of the independent variable is interpreted in exactly the same manner as it is for linear regression.</p>

<p>A normal regression graph generally looks like this:</p>

<p><img class="center" src="/images/glm/normal.png" alt="Generic Graph of Normal Regression" /></p>

<h4 id="poisson-regression">Poisson Regression</h4>

<p>A dataset often posseses a Poisson distribution when the data is measured by taking a very large number of trials, each with a small probability of success. For example, the number of earthquakes taking place in a region per year. It is mainly used in case of count data and contingency tables. Binomial distributions often converge into Poisson when the number of cases(n) is large and probability of success(p) small.</p>

<p>The poisson is completely defined by the rate parameter <script type="math/tex">\lambda</script>. The link function is <script type="math/tex">ln(y) = x*\beta_{1}</script>, which can be written as <script type="math/tex">y = e^{x*\beta_{1}}</script>. Because the link function is logarithmic, it is also referred to as log-linear regression.</p>

<p>The meaning of the co-efficient in the case of poisson is “for increase 1 of <em>x</em>, <em>y</em> changes <script type="math/tex">y = e^\beta_{1}</script> times.”.</p>

<p>A poisson graph looks something like this:</p>

<p><img class="center" src="/images/glm/poisson.png" alt="Graph of Poisson Regression" /></p>

<h4 id="probit-regression">Probit Regression</h4>

<p>Probit is used for modeling binary outcome varialbles. Probit is similar to  logit, the choice between the two largely being a matter of personal preference.</p>

<p>In the probit model, the inverse standard normal distribution of the probability is modeled as a linear combination of the predictors (in simple terms, something like <script type="math/tex">y = \Phi(\beta_{0} + x_{1}*\beta_{1}...)</script> , where <script type="math/tex">\Phi</script> is the CDF of the standard normal). Therefore, the link function can be written as <script type="math/tex">z = \Phi^{-1}(p)</script> where <script type="math/tex">\Phi(z)</script> is the standard normal cumulative density function (here <em>p</em> is probability of the occurence of a random variable <em>x</em> and <em>z</em> is the z-score of the y value).</p>

<p>The fitted mean values of the probit are calculated by setting the upper limit of the normal CDF integral as <script type="math/tex">x*\beta_{1}</script>, and lower limit as <script type="math/tex">-\infty</script>. This is so because evaluating any normally distributed random number over its CDF will yield the probability of its occurence, which is what we expect from the fitted values of a probit.</p>

<p>The coefficient of <em>x</em> is interpreted as “one unit change in <em>x</em> leads to a change <script type="math/tex">\beta_{1}</script> in the z-score of <em>y</em>”.</p>

<p>Looking at the graph of probit, one can see the similarities between logit and probit:</p>

<p><img class="center" src="/images/glm/probit.png" /></p>

<h2 id="finding-the-coefficients-of-a-glm">Finding the coefficients of a GLM</h2>

<p>There are two major methods of finding the coefficients of a GLM:</p>

<ul>
  <li>Maximum Likelihood Estimation (MLE).</li>
  <li>Iteratively Reweighed Least Squares (IRLS).</li>
</ul>

<h4 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h4>

<p>The most obvious way of finding the coefficients of the given regression analysis is by maximizing the likelihood function of the distribution that the independent variables belong to. This becomes much easier when we take the natural logarithm of the likelihood function. Hence, the name ‘Maximum Likelihood Estimation’. The Newton-Raphson method is used to this effect for maximizing the beta values (coefficients) of the log likelihood function.</p>

<p>The first derivative of the log likelihood wrt to <script type="math/tex">\beta</script> is calculated for all the <script type="math/tex">x_{i}</script> terms (this is the jacobian matrix), and so is the second derivative (this is the hessian matrix). The coefficient is estimated by first choosing an initial estimate for <script type="math/tex">x_{old}</script>, and then iteratively correcting this initial estimate by trying to bring the equation</p>

<script type="math/tex; mode=display">\begin{align}
x_{new} = x_{old} - inverse(hessian)*jacobian   ..(1) 
\end{align}</script>

<p>to equality (with a pre-set tolerance level). A good implementation of MLE can be found <a href="http://petertessin.com/MaxLik.pdf">here</a>.</p>

<h4 id="iteratively-reweighed-least-squares">Iteratively Reweighed Least Squares</h4>

<p>Another useful but somewhat slower method of estimating the regression coefficients of a dataset is Iteratively Reweighed Least Squares. It is slower mainly because of the number of co-efficients involved and the somewhat extra memory that is taken up by the various matrices used by this method. The upside of IRLS is that it is very easy to implement as is easily extensible to any kind of GLM.</p>

<p>The IRLS method also ultimately boils to the equation of the Newton Raphson (1), but the key difference between the two is that in MLE we try to maximize the likelihood but in IRLS we try to minimize the errors. Therefore, the manner in which the hessian and jacobian matrices are calculated is quite different. The IRLS equation is written as:</p>

<script type="math/tex; mode=display">\begin{align}
    b_{new} = b_{old} - inverse(X'*W*X)*(X'*(y - \mu))
\end{align}</script>

<p>Here, the hessian matrix is <script type="math/tex">-(X'*W*X)</script> and the jacobian is <script type="math/tex">(X'*(y - \mu))</script>. Let’s see the significance of each term in each of these matrices:</p>

<ul>
  <li><em>X</em> - The matrix of independent variables  <script type="math/tex">x_{1}, x_{2},...</script> alongwith the constant vector.</li>
  <li><em>X’</em> - Transpose of X.</li>
  <li><em>W</em> - The weight matrix. This is the most important entity in the equation and understanding it completely is paramount to gaining an understanding of the IRLS as whole.
    <ul>
      <li>The <em>weight</em> matrix is present to reduce favorism of the best fit curve towards larger values of x. Hence, the weight matrix acts as a mediator of sorts between the very small and very large values of x (if any). It is a diagonal matrix with each non-zero value representing the weight for each vector <script type="math/tex">x_{i}</script> in the sample data.</li>
      <li>Calculation of the weight matrix is dependent on the probability distribution shown by the independent random variables. The weight expression can be calculated by taking a look at the equation of the hessian matrix. So in the case of logistic regression, the weight matrix is a diagonal matrix with the ith entry as <script type="math/tex">p(x_{i}, \beta_{old})*(1 - p(x_{i}, \beta_{old}))</script>.</li>
      <li>The W matrix is (the inverse?) of the variance/covariance matrix. On logistic and Poisson regression, the variance on each case depend on the mean, so that is the meaning of <script type="math/tex">p(x_{i}, \beta_{old})*(1 - p(x_{i}, \beta_{old}))</script>.</li>
    </ul>
  </li>
  <li><script type="math/tex">(y - \mu)</script> - This is a matrix whose ith value the is difference between the actual corresponding value on the y-axis minus <script type="math/tex">\mu = x*b_{old}</script>. The value of this term is crucial in determining the error with which the coefficients have been calculated. Frequently an error of 10e-4 is acceptable.</li>
</ul>

<h2 id="generalized-linear-models-in-ruby">Generalized Linear Models in Ruby</h2>

<p>Calculating the co-efficients and a host of other properties of a GLM is extremely simple and intuitive in Ruby. Let us see some examples of GLM by using the <code>daru</code> and <code>statsample-glm</code> gems:</p>

<p>First install <code>statsample-glm</code> by running <code>gem install statsample-glm</code>, statsample will be downloaded alongwith it if it is not installed directly. Then download the CSV files from <a href="https://github.com/SciRuby/statsample-glm/blob/master/spec/data/logistic_mle.csv">here</a>.</p>

<p>Statsample-glm supports a variety of GLM methods, giving the choice of both, IRLS and MLE algorithms to the user for almost every distribution, and all this through a simple and intutive API. The primary calling function for all distribtions and algorithms is <code>Statsample::GLM.compute(data_set, dependent, method, options)</code>. We specify the data set, dependent variable, type of regression and finally an options hash in which one can specify a variety of customization options for the computation.</p>

<p>To compute the co-efficients of a logistic regression, try this code:</p>

<div class="language-ruby highlighter-coderay"><table class="CodeRay"><tr>
  <td class="line-numbers"><pre><a href="#n1" name="n1">1</a>
<a href="#n2" name="n2">2</a>
<a href="#n3" name="n3">3</a>
<a href="#n4" name="n4">4</a>
<a href="#n5" name="n5">5</a>
<a href="#n6" name="n6">6</a>
<a href="#n7" name="n7">7</a>
<a href="#n8" name="n8">8</a>
<a href="#n9" name="n9">9</a>
<strong><a href="#n10" name="n10">10</a></strong>
<a href="#n11" name="n11">11</a>
<a href="#n12" name="n12">12</a>
<a href="#n13" name="n13">13</a>
<a href="#n14" name="n14">14</a>
<a href="#n15" name="n15">15</a>
<a href="#n16" name="n16">16</a>
<a href="#n17" name="n17">17</a>
</pre></td>
  <td class="code"><pre>require <span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">daru</span><span style="color:#710">'</span></span>
require <span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">statsample-glm</span><span style="color:#710">'</span></span>
<span style="color:#777"># Code for computing coefficients and related attributes of a logistic regression.</span>

data_set = <span style="color:#036;font-weight:bold">Daru</span>::<span style="color:#036;font-weight:bold">DataFrame</span>.from_csv <span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">&quot;</span><span style="color:#D20">logistic_mle.csv</span><span style="color:#710">&quot;</span></span>
glm = <span style="color:#036;font-weight:bold">Statsample</span>::<span style="color:#036;font-weight:bold">GLM</span>.compute data_set, <span style="color:#A60">:y</span>, <span style="color:#A60">:logistic</span>, {<span style="color:#606">constant</span>: <span style="color:#00D">1</span>, <span style="color:#606">algorithm</span>: <span style="color:#A60">:mle</span>} 

<span style="color:#777"># Options hash specifying addition of an extra constants </span>
<span style="color:#777"># vector all of whose values is '1' and also specifying </span>
<span style="color:#777"># that the MLE algorithm is to be used.</span>

puts glm.coefficients   
  <span style="color:#777">#=&gt; [0.3270, 0.8147, -0.4031,-5.3658]</span>
puts glm.standard_error
  <span style="color:#777">#=&gt; [0.4390, 0.4270, 0.3819,1.9045]</span>
puts glm.log_likelihood 
  <span style="color:#777">#=&gt; -38.8669</span>
</pre></td>
</tr></table>
</div>

<p>Similar to the above code, you can try implementing poisson, normal or probit regression models and use the data files from the link above as sample data. Just go through the tests in the source code on GitHub or read the documentation for further details and feel free to drop me a mail in case you have any doubts/suggestions for improvements.</p>

<p>Cheers!</p>

<hr />

<h6 id="further-reading">Further Reading</h6>
<ul>
  <li><a href="https://cise.ufl.edu/class/cis6930sp10esl/downloads/LogisticRegression.pdf">A good explanation of IRLS</a>.</li>
  <li><a href="http://www.stat.cmu.edu/~cshalizi/402/lectures/14-logistic-regression/lecture-14.pdf">Logistic Regression and Newtons Method</a>.</li>
  <li><a href="https://files.nyu.edu/mrg217/public/mle_introduction1.pdf">A good resource on the how and why behind the calculation of standard errors</a>.</li>
  <li><a href="http://www.columbia.edu/~so33/SusDev/Lecture_9.pdf">Logit and Probit</a>.</li>
  <li><a href="http://www.nesug.org/Proceedings/nesug10/sa/sa04.pdf">A very good explanation of the Poisson regression</a>.</li>
</ul>
</div>


  <footer>
    <p class="meta">
      
<span class="byline author vcard">Posted by <span class="fn">Sameer Deshmukh</span></span>

      





      


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://v0dro.github.io/blog/2014/09/21/code-generalized-linear-models-introduction-and-implementation-in-ruby/" data-via="v0dro" data-counturl="http://v0dro.github.io/blog/2014/09/21/code-generalized-linear-models-introduction-and-implementation-in-ruby/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2014/08/27/code-managing-large-open-source-projects-for-beginners/" title="Previous Post: [code]Managing Large Open Source Projects: for Beginners.">&laquo; [code]Managing Large Open Source Projects: for Beginners.</a>
      
      
        <a class="basic-alignment right" href="/blog/2014/09/28/travel-hampi-bengaluru-allepy-part-2/" title="Next Post: [travel] Hampi-Bengaluru-Allepy Part 2">[travel] Hampi-Bengaluru-Allepy Part 2 &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2017/10/23/explanation-of-exafmm-learning-codes/">Explanation of ExaFMM Learning Codes.</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/11/24/gsoc-2016-wrap-up-for-sciruby/">GSOC 2016 Wrap Up for SciRuby</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/08/27/advice-for-future-gsoc-students-and-mentors-based-on-my-experience/">Advice for Future GSOC Students and Mentors Based on My Experience.</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/08/21/setting-up-a-lexical-analyser-and-parser-in-ruby/">Setting Up a Lexical Analyser and Parser in Ruby</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/08/16/random-thoughts-on-music-theory/">Random Thoughts on Music Theory.</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <a href="https://github.com/v0dro">@v0dro</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'v0dro',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - Sameer Deshmukh -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'v0dro';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://v0dro.github.io/blog/2014/09/21/code-generalized-linear-models-introduction-and-implementation-in-ruby/';
        var disqus_url = 'http://v0dro.github.io/blog/2014/09/21/code-generalized-linear-models-introduction-and-implementation-in-ruby/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
